---
title: "Life and Death of Great Open Source Projects"
author:
  - Jianchao Yang <yang.jianc@husky.neu.edu>
  - Zexi Han <hhan.ze@husky.neu.edu>
date: '`r Sys.Date()`'
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
    toc: yes
  # html_notebook:
  #   fig_caption: yes
  #   keep_tex: yes
  #   latex_engine: xelatex
  #   number_sections: yes
  #   toc: yes
  html_document:
    toc: yes
subtitle: An exploratory analysis on the activity patterns of GitHub repositories
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, warning = FALSE, 
  cache = TRUE,
  echo = FALSE, eval = TRUE, tidy = FALSE,
  fig.width = 8,
  fig.height = 4
)
options(htmltools.dir.version = FALSE)
library(stringr)
library(dplyr)
library(magrittr)
library(lubridate)
library(readr)
library(ggplot2)
library(feather)
library(reshape2)
library(knitr)
```

```{r, eval = FALSE}
# generate rmarkdown manually so we can reuse environment variables
rmarkdown::render("final.Rmd")
```

```{r, eval=FALSE}
source("include/init.R")
n_total <- nrow(available_repos)
```

```{r}
if (!exists("g_contributors")) {
  # we have a sample of our database (2,500 repos)
  # downloaded and written as fether files
  g_contributors <- read_feather("data/g_contributors.feather")
  g_repo <- read_feather("data/g_repo.feather")
  g_languages <- read_feather("data/g_languages.feather")
  g_issues <- read_feather("data/g_issues.feather")
  g_issue_comments <- read_feather("data/g_issue_comments.feather")
  g_issue_events <- read_feather("data/g_issue_events.feather")
  g_stargazers <- read_feather("data/g_stargazers.feather")
}
```

# Introduction

With its unique and user-friendly social coding features such as issues tracking, forks,  pull requests, commit comments and wikis, GitHub has deservedly become the most popular source code hosting service in the world. Many open source developers use GitHub not only for source code management, but also to collaborate with fellow developers, share knowledge, or simply showcase their personal work. The vibrant and all-encompassing online community of GitHub makes its data a prime window on the social dynamics of open source development.

This project is inspired by the magnitude and heterogeneity of GitHub’s project activity data. Our objective is to identify activity patterns for different types of open source projects and pinpoint indicators and determining factors that are most directly related to these patterns. We will do this by first exploring activity patterns using a Shiny app, then split projects into 4 groups based on codebase size (tiny, small, medium, large), and examine for each group how did their community interest (measured by changes in number of stars), maintainer commitment (number of commits) and community involvement (number of issues and issue comments) unfold over time.

# Methods

## Sampling and Settings

Among the 57 million repositories GitHub is hosting ^[As of data on April 19, 2017], a large portion of them are forks or small personal projects with almost no outside visibility. In order to make the data relevant and analyzable, we shall pick only repositories of adequate community interests and values. And to make sure we still cover all different kinds of repositories, we are selecting the top 1% most starred original repositories (i.e., those were not forks) of each programming language, using GHTorrnet MySQL database dumps exported on April 1, 2017. Languages with less than 100 repositories were ignored, leaving us with 36,068 repositories in 228 languages.

## Measurements of Repository Activity

There are three major type of repository activities: repository starring, code commits and issues. For issues, there are also issue events and issue comments. Other types of activities also include commit comments, release downloads, milestones, etc. For simplicity, we collected only the major acticities: starring, commit counts, and issues (including issue events and comments).

Most of the activities can be aggregated to weekly counts. By looking at the changes of these counts over time, we get a rough picture of how much effort the maintainers have committed to the project, how fast a project gained interests from the community, and how deep the community was involved. These measurements are the most important activities that can happen for a repository on GitHub.

In addition to weekly counts of various activity events, we have also built two performance metrics for evaluating maitainer commitment.

### Maintainer Commitment

**Number of commits**

A commit is an update to the files in the codebase. Number of weekly commits is a direct way of measuring how much efforts the contributors have put into the repository. It will be optimal if we can distinguish commits from core maintainers and the general public. Since that involves scraping one more dataset and this was not planned when we started the project, we are not going to make such separation of authors.

**Issues response time**

Issues response time is the average time needed for a repository maitainer to take action on an issue opened by someone else. The faster the response, the more commited the maitainer is.

**Issues close time**

Issues close time is the duration between when the issues were created and when they were last closed, i.e., how fast an issue gets resolved. The faster the close time, the more responsive the maitainers are.

### Community Engagement

**Issues count**

Issues can be reported by anyone. For open source projects on GitHub, issues are an essential channel where the community communicate with the authors: filing bugs, asking feature requests/suggestions, getting support in using the code, and so on.

**Issue comments count**

Higher number of comments on a single issue indicates more heated debate, which can be simply because of the complexity of the problem, but may also indicate the popularity of a project.

**Pull requests count**

Pull requests are contributions from the community. More pull requests means more outside people are involved in the development of the projects, often making them more resilient against faded maitainer interest.

### Community Interests

**Number of stargazers**

GitHub officially admits number of stars as "an approximate level of interest" ^[https://developer.github.com/v3/activity/starring/]. We have the possibility to know when a user starred a project. Aggregating a weekly count for the increase in stargazers can be used as an indication of how community interest changes over time.

## Data Collection

The [GHTorrent](http://ghtorrent.org/) data were used only for generating this initial seed of potentially “great” projects, then all other data were scraped directly from GitHub API. Since repositories change owner and names from time to time, and GitHub sometimes handle such case with redirections, we have inevitably encountered a few 404s as well as scraped some duplicate data. To reduce the impact of repository renaming and ownership transfers to minimal, we used the seed only for scraping repository details, then used the up to date owner logins and repository names from the repository details to scrape all other data. ^[The actual ramifications were more nuanced than this. We didn’t realize how serious this problem was before we have scraped all data, which means we had to run several SQL queries to identify those who changed names and scrape them again.]

With data at such a large scale, we put our scraping program on a cloud server (AWS EC2 instance) so it can run overnight. To maximize performance and efficiency, we use MySQL to store the scraped data and are saving data on the fly while scraping. In addition to that, each time a data point was successfully saved, an near empty text file will be created in the file system and it will later be used to skip scraped data in case of servie interruptions and system fault.

At the beginning, we tried to write scraped data into the database on the fly, and didn't find a way to efficiently check whether some data has been scraped or not. Then we decide to srape all data to local files then import then to the database. For the 34,779 repositories that were still alive, we scraped 8.9G of issues data, 4.6G issue events, and 14G issue comments data (more than 30G data in total), which took us about 2 days to scrape and 20 hours to import into MySQL and build the indexes (not including time of debugging).

Out of extensive struggles and testing, we found that the most practical AWS configuration with a bearable speed is to use a c4.large (4 cores CPU, 7.5G RAM) EC2 instance type while scraping and a 100GB provisioned SSD Elastic Block Storage with at least 1,500 iops while importing data.

The scraping process has been redesigned since then--we returned back to write data on the fly, but also used small files to check whether a data point has been scraped or not and would not add database indexes afte all data are scraped.

## Data Exporation

To better understand the data we collected, we created a [Shiny dashboard](http://shiny.yjc.me/github-life/), where a user can conveniently explore the activity timeline of a repository, including how number of issues, stargazers and commits (by different authors) evovled over time.

If time permits, we may add more features to the dashboard, such as issues timeline: response rate, response time, resolve time, community contribution rate (commits of non-core-maintainers) etc; and aggregated measurements by different repository groups: organization, programming language, lanuage of users, region, repository objective (library, framework, cheatsheat, references) etc.

But with this simple combined activity timeline, we can already see a lot of different patterns for different projects--basically no two projects are the same. Some showed clear regular development cycles (Figure. 1 twbs/bootstrap); some got their initial fame very fast, but also died fast--this mostly happens to "cheatsheet" type of repositories (Figure. 2 vhf/free-programming-books); some are seeing very steady acticities as community interests keep growing (Figure 3. golang/go).

![twbs/bootstrap](images/bootstrap.png)

![vhf/free-programming-books](images/freebook.png)

![golang/go](images/go.png)

Our goal would be to detect these patterns and build a typology to describe them.

## Grouping and Classification

To understand the implications of these activity patterns and the correlations between different metrics, we are splitting repositories into groups and comparing how various metrics behave differently over groups.

We sampled 2,500 repositories out of 34,779, and split them into 4 equal-sized groups based on their codebase sizes. This gives us approxiamtely 620 repositories each group, and each across the whole spectrum of programming languages and community interest (stargazers count).

As we are mainly looking at temporal data, it would be more analyzable if every repository had the same time length of data. So the first thing we did is to filter out repositories younger than __one year__ and only look at the first year data for each remaining repositories. This time range is long enough for us to discover patterns, but also avoids dropping too many repositories. ^[One more caveat is that repositories created at different years might have a totally different pattern, as GitHub was getting more and more popular and its user base changes. But for simplicity reason, we are not ignoring such effects for now.].

```{r}
FillEmptyWeeks <- function(dat, mindate, maxdate) {
  if (nrow(dat) == 0) return(dat)
  if (any(is.na(dat$week))) return(dat) 
  dat.full <- data.frame(week = seq(mindate, maxdate, 7)) %>%
    full_join(dat, by = "week") 
  dat.full[is.na(dat.full)] <- 0
  dat.full
}

# subset by age
num_weeks <- 52
# create a list of repos that age above num_weeks
repo_ages <- g_contributors %>% 
  distinct(repo, week) %>% 
  # count for each repo how many distinct weeks it have
  count(repo)

# selected repos that meet the age requirement 
year_selected <- repo_ages %>%
  subset(n >= num_weeks) %>%
  .$repo
```

```{r}
# subset by size
size_selected <- g_repo %>% 
  mutate(repo = paste0(owner_login,"/",name)) %>% 
  subset(repo %in% year_selected) %>% 
  select(repo,size)
```

# Data Analysis

## Overview

```{r, eval = FALSE}
# quantile(g_repo$size)
```

Among the 2,500 repositories we sampled, nearly half had less then 5MB of codebase.

```{r}
# repo size density
fill <- "#4271AE"
line <- "#1F3552"
ggplot(g_repo, aes(size)) +
  geom_density(fill = fill, colour = line, alpha = 0.6) +
  scale_x_continuous(limits = c(0,30000)) +
  labs(
    x="Repo Size (Bytes)",
    y="Density",
    title="Density Plot of Repository Size"
  )
```

```{r}
size_quantiles <- quantile(size_selected$size, probs = seq(0, 1, 0.25))
size_quantiles %>%
  t() %>%
  knitr::kable(caption = "Repo size quantiles")
```

## Commits

```{r}
contributions_selected <- g_contributors %>% 
  # subset repos that age above num_weeks
  subset(repo %in% year_selected) %>% 
  # sum commits count by different collaborators
  group_by(repo, week) %>% 
  summarise(variable=sum(commits)) %>% 
  ungroup() %>% 
  # subset by num_weeks
  group_by(repo) %>% 
  mutate(id = 1:n()) %>% 
  ungroup() %>% 
  subset(id %in% 2:num_weeks)

# separate groups
contributions_tiny <- contributions_selected %>% 
  left_join(size_selected, by = "repo") %>% 
  subset(size >= size_quantiles[[1]] & size < size_quantiles[[2]])
contributions_small <- contributions_selected %>% 
  left_join(size_selected, by = "repo") %>% 
  subset(size >= size_quantiles[[2]] & size < size_quantiles[[3]])
contributions_medium <- contributions_selected %>% 
  left_join(size_selected, by = "repo") %>% 
  subset(size >= size_quantiles[[3]] & size < size_quantiles[[4]])
contributions_large <- contributions_selected %>% 
  left_join(size_selected, by = "repo") %>% 
  subset(size >= size_quantiles[[4]] & size < size_quantiles[[5]])

projects_tiny <- unique(contributions_tiny$repo)
projects_small <- unique(contributions_small$repo)
projects_medium <- unique(contributions_medium$repo)
projects_large <- unique(contributions_large$repo)
projects_selected <- c(projects_tiny,projects_small,projects_medium,projects_large)

# aggregate by variable mean
aggr_ct <- contributions_tiny %>% 
  group_by(id) %>% 
  summarise(average_variable = mean(variable)) %>% 
  mutate(group = "Tiny")
aggr_cs <- contributions_small %>% 
  group_by(id) %>% 
  summarise(average_variable = mean(variable)) %>% 
  mutate(group = "Small")
aggr_cm <- contributions_medium %>% 
  group_by(id) %>% 
  summarise(average_variable = mean(variable)) %>% 
  mutate(group = "Medium")
aggr_cl <- contributions_large %>% 
  group_by(id) %>% 
  summarise(average_variable = mean(variable)) %>% 
  mutate(group = "Large")

aggr_c <- rbind(aggr_ct,aggr_cs,aggr_cm,aggr_cl)

# time-series plot
ggplot(aggr_c, aes(id,average_variable,color=group)) +
  geom_line() +
  labs(
    x="Number of weeks",
    y="Number of commits",
    title="Average Number of Weekly Commits",
    color="Group"
  )
```

Smaller projects naturally have less commits, but what is more interesting is that almost all groups had a decline in number of commits since the creation, but bigger projects tend to have a more steady stream of code changes, and the number of weekly commits would actually increase as time goes by.

## Issues

### Issues response time

Almost for all repositories, issue reponse time increases over time, regardless of the codebase size. One explaination is that maintainers were more active when their projects were first made public, or in the early stage of development, they are using issues to track working progress, which implies more frequent updates to the issues.

For tiny projects, the increase in reponse time is especially worse, indicating a sense of abandonment. On the contrary, issues response time for large repositories turned more stable in the long run, and even dipped till the end.

```{r}
# get the time when the first comment created in each issue
first_comments_tmp <- g_issue_comments %>% 
  select(repo, issue_number, created_at) %>% 
  arrange(repo, issue_number, created_at) %>% 
  distinct(repo, issue_number, .keep_all=TRUE)

first_comments <- first_comments_tmp %>% 
  left_join(g_issues, by=c("repo"="repo","issue_number"="number")) %>% 
  select(repo,issue_number,created_at.x,created_at.y,comments)

first_comments$created_at.x <- ymd_hms(first_comments$created_at.x)
first_comments$created_at.y <- ymd_hms(first_comments$created_at.y)

# separate groups
issues_response_selected <- first_comments %>% 
  mutate(time = difftime(created_at.x,created_at.y,units="hours")) %>% 
  # aggregate by week
  mutate(year = year(created_at.x), week = week(created_at.x)) %>% 
  group_by(repo, year, week) %>% 
  summarise(variable = mean(time,na.rm=TRUE)) %>% 
  ungroup() %>% 
  # subset by num_weeks
  group_by(repo) %>% 
  mutate(id = 1:n()) %>% 
  ungroup() %>% 
  subset(id %in% 2:num_weeks)

# aggregate by variable mean
aggr_by_size <- function(df_selected){
  df_tiny <- df_selected %>% subset(repo %in% projects_tiny)
  df_small <- df_selected %>% subset(repo %in% projects_small)
  df_medium <- df_selected %>% subset(repo %in% projects_medium)
  df_large <- df_selected %>% subset(repo %in% projects_large)

  aggr_tiny <- df_tiny %>% 
    group_by(id) %>% 
    summarise(average_variable = mean(variable)) %>% 
    mutate(group = "Tiny")
  aggr_small <- df_small %>% 
    group_by(id) %>% 
    summarise(average_variable = mean(variable)) %>% 
    mutate(group = "Small")
  aggr_medium <- df_medium %>% 
    group_by(id) %>% 
    summarise(average_variable = mean(variable)) %>% 
    mutate(group = "Medium")
  aggr_large <- df_large %>% 
    group_by(id) %>% 
    summarise(average_variable = mean(variable)) %>% 
    mutate(group = "Large")
  
  df_aggr <- rbind(aggr_tiny,aggr_small,aggr_medium,aggr_large)
  return(df_aggr)
}

aggr_ir <- aggr_by_size(issues_response_selected)

# time-series plot
ggplot(aggr_ir, aes(id,average_variable,color=group)) +
  geom_point() +
  geom_smooth() +
  labs(
    x="Number of weeks",
    y="Time (hour)",
    title="Average Issues Response Time",
    color="Group"
  )
```

### Issues close time

Issue close time have downward trend for most projects, which might be because of different issue types as of different stages a repository is in. At the begining, more feature request type of isssues are created, which in general requires mroe time to resolve; while on later stages, more issues would be bug reports or general questions, which requires relatively little efforts. 

In addition, the bigger the codebase size is, the longer it is required to resolve an issue, which might be related to the complexity of the code.


```{r}
g_issues$created_at <- ymd_hms(g_issues$created_at)
g_issues$closed_at <- ymd_hms(g_issues$closed_at)
issues_time_selected <- g_issues %>% 
  subset(state=="closed") %>% 
  mutate(time = difftime(closed_at,created_at,units="hours")) %>% 
  # aggregate by week
  mutate(year = year(created_at), week = week(created_at)) %>% 
  group_by(repo, year, week) %>% 
  summarise(variable = mean(time,na.rm=TRUE)) %>% 
  ungroup() %>% 
  # subset num_weeks
  group_by(repo) %>% 
  mutate(id = 1:n()) %>% 
  ungroup() %>% 
  subset(id %in% 2:num_weeks)

# aggregate by variable mean
aggr_ic <- aggr_by_size(issues_time_selected)

# time-series plot
ggplot(aggr_ic, aes(id,average_variable,color=group)) +
  geom_point() +
  geom_smooth() +
  labs(
    x="Number of weeks",
    y="Time (hour)",
    title="Average Issues Closed Time",
    color="Group"
  )
```

### Issues count

Issues created by non-maintainers indicate involvements of the community, with pull requests especially so.

```{r}
issues_selected <- g_issues %>% 
  subset(repo %in% projects_selected) %>% 
  # aggregate by week
  mutate(year = year(created_at), week = week(created_at)) %>% 
  group_by(repo, year, week) %>% 
  summarise(variable = n()) %>% 
  ungroup() %>% 
  # subset num_weeks
  group_by(repo) %>% 
  mutate(id = 1:n()) %>% 
  ungroup() %>% 
  subset(id %in% 2:num_weeks)

# aggregate by variable mean
aggr_i <- aggr_by_size(issues_selected)

# time-series plot
ggplot(aggr_i, aes(id,average_variable,color=group)) +
  geom_line() +
  labs(
    x="Number of weeks",
    y="Number of issues",
    title="Average Number of Weekly Issues",
    color="Group"
  )
```

Large repositories have a very obvious upward trend in terms of new issues created every week, while repositories of other sizes stay flat most of the time. There are two spikes in this graph, but we are not too worry about them since we have a relatively small sample size, and the noise can get very obvious.

### Issue comments

Issue comments are very in line with new issues count. They even showed the same spikes caused by noise.

```{r}
g_issue_comments$created_at <- ymd_hms(g_issue_comments$created_at)
issue_comments_selected <- g_issue_comments %>% 
  subset(repo %in% projects_selected) %>% 
  # aggregate by week
  mutate(year = year(created_at), week = week(created_at)) %>% 
  group_by(repo, year, week) %>% 
  summarise(variable = n()) %>% 
  ungroup() %>% 
  # subset num_weeks
  group_by(repo) %>% 
  mutate(id = 1:n()) %>% 
  ungroup() %>% 
  subset(id %in% 2:num_weeks)

# aggregate by variable mean
aggr_ic <- aggr_by_size(issue_comments_selected)

# time-series plot
ggplot(aggr_ic, aes(id,average_variable,color=group)) +
  geom_line() +
  labs(
    x="Number of weeks",
    y="Number of issue comments",
    title="Average Number of Weekly Issue Comments",
    color="Group"
  )
```

### Pull requests

Large repository can attracts more contributions from the community to assist its development. Comparing to general issues, pull requests are showing a more obvious staircase pattern.  We might need to investigate why is the case in the future work.

```{r}
pullrequests_selected <- g_issues %>% 
  subset(is_pull_request == 1) %>% 
  subset(repo %in% projects_selected) %>% 
  # aggregate by week
  mutate(year = year(created_at), week = week(created_at)) %>% 
  group_by(repo, year, week) %>% 
  summarise(variable = n()) %>% 
  ungroup() %>% 
  # subset num_weeks
  group_by(repo) %>% 
  mutate(id = 1:n()) %>% 
  ungroup() %>% 
  subset(id %in% 2:num_weeks)

# aggregate by variable mean
aggr_pr <- aggr_by_size(pullrequests_selected)

# time-series plot
ggplot(aggr_pr, aes(id,average_variable,color=group)) +
  geom_line() +
  labs(
    x="Number of weeks",
    y="Number of pull requests",
    title="Average Number of Weekly Pull Requests",
    color="Group"
  )
```

## Stargazers

Changes in number of stars are less correlated with repository sizes. It seems that projects tend to gain more attraction when they first released, as all groups showed more stars during the first few weeks.

```{r}
g_stargazers$starred_at <- ymd_hms(g_stargazers$starred_at)
stargazers_selected <- g_stargazers %>% 
  subset(repo %in% projects_selected) %>% 
  # aggregate by week
  mutate(year = year(starred_at), week = week(starred_at)) %>% 
  group_by(repo, year, week) %>% 
  summarise(variable = n()) %>% 
  ungroup() %>% 
  # subset num_weeks
  group_by(repo) %>% 
  mutate(id = 1:n()) %>% 
  ungroup() %>% 
  subset(id %in% 2:num_weeks)

# aggregate by variable mean
aggr_s <- aggr_by_size(stargazers_selected)

# time-series plot
ggplot(aggr_s, aes(id,average_variable,color=group)) +
  geom_line() +
  labs(
    x="Number of weeks",
    y="Number of stargazers",
    title="Average Number of Weekly Stargazer Increments",
    color="Group"
  )
```

## Aggregated Measurements

Besides time series exploration, it would be also helpful to compare the total and final state aggregated measurements for each group.

### Mean & Variance of Measurements

```{r}
generate_box_part <- function(tiny_data,small_data,medium_data,large_data,measurement_chr){
  box_t <- tiny_data %>% 
    group_by(repo) %>% 
    summarise(count=sum(variable)) %>% 
    mutate(measurement=measurement_chr, group = "Tiny")
  box_s <- small_data %>% 
    group_by(repo) %>% 
    summarise(count=sum(variable)) %>%
    mutate(measurement=measurement_chr, group = "Small")
  box_m <- medium_data %>% 
    group_by(repo) %>% 
    summarise(count=sum(variable)) %>%
    mutate(measurement=measurement_chr, group = "Medium")
  box_l <- large_data %>% 
    group_by(repo) %>% 
    summarise(count=sum(variable)) %>%
    mutate(measurement=measurement_chr, group = "Large")
  box_part <- rbind(box_t,box_s,box_m,box_l)
  return(box_part)
}

issues_tiny <- issues_selected %>% subset(repo %in% projects_tiny)
issues_small <- issues_selected %>% subset(repo %in% projects_small)
issues_medium <- issues_selected %>% subset(repo %in% projects_medium)
issues_large <- issues_selected %>% subset(repo %in% projects_large)

stargazers_tiny <- stargazers_selected %>% subset(repo %in% projects_tiny)
stargazers_small <- stargazers_selected %>% subset(repo %in% projects_small)
stargazers_medium <- stargazers_selected %>% subset(repo %in% projects_medium)
stargazers_large <- stargazers_selected %>% subset(repo %in% projects_large)

box_commits <- generate_box_part(contributions_tiny,contributions_small,
                           contributions_medium,contributions_large,"Commits")
box_issues <- generate_box_part(issues_tiny,issues_small,
                           issues_medium,issues_large,"Issues")
box_stargazers <- generate_box_part(stargazers_tiny,stargazers_small,
                           stargazers_medium,stargazers_large,"Stargazers")
box_data <- rbind(box_commits,box_issues,box_stargazers)

# create boxplot
ggplot(box_data,aes(measurement,count,color=group)) +
  geom_boxplot() +
  scale_y_continuous(limits=c(0,2500)) +
  labs(
    x="Measurement",
    y="Count",
    title="Mean & Variance of Three Measurements",
    color="Group"
  )
```

In the aggregated analysis, we simply select number of commits, issues and stargazers to represent the three measurements. The above box plot clearly shows the mean and variance of three measurements. We dropped many outliers in this plot to zoom in the boxes. For commits and issues, both mean and variance of count increase with the increase of codebase size, which indicats that more variations exist in the development pattern of larger projects. While for stargazers, there is no obvious realation between the number of stargazers and codebase size. 

### Maintainer Commitment vs. Community Engagement

As mentioned before, commits and issues reflect maitainer commitment and community engagement respectively. We want to examine how these two correlate with each other, using the current total numbers of these two types of events for each repository.

```{r}
pair_commits <- contributions_selected %>% 
  group_by(repo) %>% 
  summarise(commits_count = sum(variable))

issues_selected %>% 
  group_by(repo) %>% 
  summarise(issues_count = sum(variable)) %>% 
  left_join(pair_commits, by = "repo") %>% 
  ggplot(aes(commits_count,issues_count)) +
    geom_point() +
    geom_density_2d() +
    scale_x_continuous(limits = c(0,800)) +
    scale_y_continuous(limits = c(0,350)) +
    labs(
      x="Commits count",
      y="Issues count",
      title="2D Density - Maintainer Commitment vs. Community Engagement"
    )
```

We put them in above 2D density plot. Each data point in the graph is a repository, and x,y axises represent its total commits and issues count in its life. We can see that the ratio of commits count and issues count is approximately 2:1. Besides, most of the repositories clustered at the contour center of 200 commits and 100 issues. In addition, the distance between same-density contour line becomes wider as both the count of commits and issues increase, which cound be interpreted as that the project with more contributions made by its maintainers can attract more supports from the community.

### Three measurements in one graph

```{r}
aggregate_group <- function(aggr_c,aggr_i,aggr_s){
  aggr_ca <- aggr_c %>% 
    mutate(measurement = "Commits", average_count = scale(average_variable)) %>% 
    select(id, measurement, average_count, group)
  aggr_ia <- aggr_i %>% 
    mutate(measurement = "Issues", average_count = scale(average_variable)) %>% 
    select(id, measurement, average_count, group)
  aggr_sa <- aggr_s %>% 
    mutate(measurement = "Stargazers", average_count = scale(average_variable)) %>% 
    select(id, measurement, average_count, group)
  aggregation_group <- rbind(aggr_ca, aggr_ia, aggr_sa)
  return(aggregation_group)
}
aggr_it <- aggr_i %>% subset(group == "Tiny")
aggr_st <- aggr_s %>% subset(group == "Tiny")
aggr_is <- aggr_i %>% subset(group == "Small")
aggr_ss <- aggr_s %>% subset(group == "Small")
aggr_im <- aggr_i %>% subset(group == "Medium")
aggr_sm <- aggr_s %>% subset(group == "Medium")
aggr_il <- aggr_i %>% subset(group == "Large")
aggr_sl <- aggr_s %>% subset(group == "Large")

aggregation_tiny <- aggregate_group(aggr_ct,aggr_it,aggr_st)
aggregation_small <- aggregate_group(aggr_cs,aggr_is,aggr_ss)
aggregation_medium <- aggregate_group(aggr_cm,aggr_im,aggr_sm)
aggregation_large <- aggregate_group(aggr_cl,aggr_il,aggr_sl)

aggregation_comparison <- rbind(aggregation_tiny,aggregation_small,aggregation_medium,aggregation_large)

ggplot(aggregation_comparison, aes(id,average_count,color=measurement)) +
  geom_point(size=.5) +
  geom_smooth(size=.8) +
  facet_wrap( ~ group, ncol = 2) +
  labs(
    x="Number of weeks",
    y="Scaled count",
    title="Comparison of Three Measurements by Codebase Size",
    color="Measurement"
  )
```

To better compare the correlated (or not correlated) trends of three different measures, we scaled the data and plotted them in one group. Looking at the trends for different codebase size group, it seems that all groups and metrics presented a pattern of starting at a high volume, decreases for a period, then stablize at some level. 

The case for large repositories was more special. The number of commits and issues would actually started growing at some later stages, indicating a continued or even enhanced support from the maintainers and the community.

# Discussions and next steps

Our main focus for this project was collecting and exploring the implications of the repository activity data on GitHub. The main finding is that no two repositories are the same. Almost all repositories showed different patterns and it is hard to categorically say which pattern group a repository belongs to. Nevertheless, by splitting repositories into groups based on codebase sizes, we were able to identify a few nuances in between different repository types.

Future works may include finding a more robust way to classify the repositories, such as in project objectives and scales. We certainly do not want to compare a pear with an apple. To dive into a subgroup and identify patterns for each group would be more meaningful work to do.

Parallel scraping was made possible with the `future` package. It can be very efficient, yet relatively hard to debug. Storing large scale data is a challenge, especially when writing into MySQL directly via R. Shiny and Plot.ly are great tools for dynamica explorations. Aggregating things in one place is a tremendously helpful way to uncover data insights.

We plan to open source this project and add more features in the summer.

Please refer to the README document on GitHub ^[https://github.com/ktmud/github-life] for how the code are structured and which improvements were planned exactly.